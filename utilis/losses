import torch
from torch.nn import functional as F
import numpy as np

def dice_loss(score, target):
    target = target.float()
    smooth = 1e-5

    loss = 0
    for i in range(target.shape[1]):
        intersect = torch.sum(score[:, i, ...] * target[:, i, ...])
        z_sum = torch.sum(score[:, i, ...] )
        y_sum = torch.sum(target[:, i, ...] )
        loss += (2 * intersect + smooth) / (z_sum + y_sum + smooth)
    loss = 1 - loss * 1.0 / target.shape[1]

    return loss

def dice_loss1(score, target):
    target = target.float()
    smooth = 1e-5
    intersect = torch.sum(score * target)
    y_sum = torch.sum(target)
    z_sum = torch.sum(score)
    loss = (2 * intersect + smooth) / (z_sum + y_sum + smooth)
    loss = 1 - loss
    return loss

def entropy_loss(p,C=2):
    ## p N*C*W*H*D
    y1 = -1*torch.sum(p*torch.log(p+1e-6), dim=1)/torch.tensor(np.log(C)).cuda()
    ent = torch.mean(y1)

    return ent




def softmax_dice_loss(score, target):


    assert score.size() == target.size(), "Score and target must have the same shape"

    # Apply softmax to the score and target logits
    score_softmax = F.softmax(score, dim=1)
    target_softmax = F.softmax(target, dim=1)

    n = score.shape[1]  # Number of classes
    dice = 0.0

    # Compute dice loss for each class
    for i in range(0, n):
        # Compute dice loss for the i-th class
        intersect = torch.sum(score_softmax[:, i, ...] * target_softmax[:, i, ...])
        z_sum = torch.sum(score_softmax[:, i, ...])
        y_sum = torch.sum(target_softmax[:, i, ...])
        dice += (2 * intersect) / (z_sum + y_sum + 1e-5)  # Adding smooth term to avoid division by zero

    mean_dice = 1 - dice / n  # Compute mean dice loss across all classes

    return mean_dice

def entropy_loss_map(p, C=2):
    ent = -1*torch.sum(p * torch.log(p + 1e-6), dim=1, keepdim=True)/torch.tensor(np.log(C)).cuda()
    return ent



def softmax_mse_loss(score, target):
    """
    Computes the Mean Squared Error (MSE) loss after applying softmax
    to both the score (predictions) and the target.

    Arguments:
    score -- the raw logits output from the model, shape: (batch_size, num_classes, height, width)
    target -- the ground truth logits or labels, shape: (batch_size, num_classes, height, width)

    Returns:
    mse_loss -- the mean squared error loss.
    """

    assert score.size() == target.size(), "Score and target must have the same shape"

    # Apply softmax to the score and target logits
    score_softmax = F.softmax(score, dim=1)
    target_softmax = F.softmax(target, dim=1)

    # Compute the MSE loss
    mse_loss = torch.mean((score_softmax - target_softmax) ** 2)

    return mse_loss
def softmax_kl_loss(input_logits, target_logits):
    """Takes softmax on both sides and returns KL divergence

    Note:
    - Returns the sum over all examples. Divide by the batch size afterwards
      if you want the mean.
    - Sends gradients to inputs but not the targets.
    """
    assert input_logits.size() == target_logits.size()
    input_log_softmax = F.log_softmax(input_logits, dim=1)
    target_softmax = F.softmax(target_logits, dim=1)

    # return F.kl_div(input_log_softmax, target_softmax)
    kl_div = F.kl_div(input_log_softmax, target_softmax, reduction='none')
    # mean_kl_div = torch.mean(0.2*kl_div[:,0,...]+0.8*kl_div[:,1,...])
    return kl_div

def symmetric_mse_loss(input1, input2):
    """Like F.mse_loss but sends gradients to both directions

    Note:
    - Returns the sum over all examples. Divide by the batch size afterwards
      if you want the mean.
    - Sends gradients to both input1 and input2.
    """
    assert input1.size() == input2.size()
    return torch.mean((input1 - input2)**2)


def refine_weight_dict_by_GA(weight_dict, site_before_results_dict, site_after_results_dict, step_size=0.1,
                             fair_metric='loss'):
    if fair_metric == 'acc':
        signal = -1.0
    elif fair_metric == 'loss':
        signal = 1.0
    else:
        raise ValueError('fair_metric must be acc or loss')

    value_list = []
    for site_name in site_before_results_dict.keys():
        value_list.append(
            site_after_results_dict[site_name][fair_metric] - site_before_results_dict[site_name][fair_metric])

    value_list = np.array(value_list)

    step_size = 1. / 3. * step_size
    norm_gap_list = value_list / np.max(np.abs(value_list))

    for i, site_name in enumerate(weight_dict.keys()):
        weight_dict[site_name] += signal * norm_gap_list[i] * step_size

    weight_dict = weight_clip(weight_dict)

    return weight_dict


def weight_clip(weight_dict):
    new_total_weight = 0.0
    for key_name in weight_dict.keys():
        weight_dict[key_name] = np.clip(weight_dict[key_name], 0.0, 1.0)
        new_total_weight += weight_dict[key_name]

    for key_name in weight_dict.keys():
        weight_dict[key_name] /= new_total_weight

    return weight_dict
